{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - HW 2\n",
    "\n",
    "## by Clarice Mottet\n",
    "\n",
    "0. **[Part 0: Set Up](#part0)**\n",
    "- **Objective**: Initialize programming environment.\n",
    "- **Tasks:***\n",
    "    - Set up libraries\n",
    "    - Initialize global variables\n",
    "    - Create functions to be used throughout notebook base on problem set up\n",
    "\n",
    "1. **[Part 1: Approximate Policy Evaluation](#part1)**\n",
    "- **Objective**: Study the performance of simple policies using TD(0) and LSTD\n",
    "- **Tasks:**\n",
    "  - TD(0) algorithm creation\n",
    "  - LSTD algorithm creation\n",
    "  - Lazy policy (TD(0) and LSTD implementation)\n",
    "  - Aggressive policy (TD(0) and LSTD implementation)\n",
    "  - Evaluate efficacy under three feature maps with $10^4, 10^5, 10^6, 10^7$\n",
    "  - Plot the resulting value functions and compare to value function results from HW1\n",
    "\n",
    "2. **[Part 2: Approximate Policy Iteration](#part2)**\n",
    "- **Objective**: implement an approximate policy iteration method\n",
    "- **Tasks:**\n",
    "  - Create approximate policy iteration method\n",
    "  - Plot value function after 10 and 100 iterations and compare to value function from Part1\n",
    "  - Compare the value function to the optimal value function from HW1\n",
    "  - Plot the service rates assigned by the final policy, interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='part0'>Part 0: Set Up</a>\n",
    "- **Objective**: Initialize programming environment.\n",
    "- **Tasks:**\n",
    "    - Set up libraries\n",
    "    - Initialize global variables\n",
    "    - Create functions to be used throughout notebook base on problem set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Set up libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_MAX = 100\n",
    "GAMMA = 0.9\n",
    "\n",
    "Q_LOW = 0.51\n",
    "Q_HIGH = 0.6\n",
    "\n",
    "COST_LOW = 0.0\n",
    "COST_HIGH = 0.01\n",
    "\n",
    "ARRIVAL_RATE = 0.5\n",
    "\n",
    "ACTION_LIST = [0, 1] #action_ind (low = 0, high = 1)\n",
    "\n",
    "list_iters = [10, 20, 50, 100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create functions to be used throughout notebook base on problem set up\n",
    "    - reward functions\n",
    "    - transition functions\n",
    "    - evaluation functions\n",
    "    - simulation functions\n",
    "    - feature maps\n",
    "    - policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reward functions\n",
    "\n",
    "def action_cost(action_ind):\n",
    "    if action_ind == 0:\n",
    "        return COST_LOW\n",
    "    else:\n",
    "        return COST_HIGH\n",
    "\n",
    "def reward_calc(x, action_ind):\n",
    "    return -((x/N_MAX)**2) - action_cost(action_ind)\n",
    "\n",
    "def reward_under_policy(pi):\n",
    "    r_pi = np.zeros(N_MAX)\n",
    "    for x in range(0, N_MAX):\n",
    "        # action_ind = None\n",
    "        for a in ACTION_LIST:\n",
    "            r_pi[x] += reward_calc(x, a)*pi[x,a]\n",
    "    return r_pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transitions functions\n",
    "\n",
    "def reduction_rate(action_ind):\n",
    "    if action_ind == 0:\n",
    "        return Q_LOW\n",
    "    else:\n",
    "        return Q_HIGH\n",
    "    \n",
    "def transition_framework():\n",
    "    #conditional matrix P(x, x_prime, a)\n",
    "    P = np.zeros((N_MAX,N_MAX,2))\n",
    "    for x in range(0,N_MAX):\n",
    "        for a in ACTION_LIST:\n",
    "            if x == 0:\n",
    "                P[x, 0, a] = (1-ARRIVAL_RATE) + (reduction_rate(a))*(ARRIVAL_RATE)\n",
    "                P[x, 1, a] = (reduction_rate(a))*(1-ARRIVAL_RATE)\n",
    "            elif x == (N_MAX-1):\n",
    "                P[x, N_MAX-2, a] = (reduction_rate(a))*(1-ARRIVAL_RATE)\n",
    "                P[x, N_MAX-1, a] = (1-reduction_rate(a)) + (reduction_rate(a))*(ARRIVAL_RATE)\n",
    "            else:\n",
    "                #(decrement, increment)\n",
    "                for x_prime in [x-1, x, x+1]:\n",
    "                    if x_prime == (x-1):\n",
    "                        #(1,0)\n",
    "                        P[x, x_prime, a] = (reduction_rate(a))*(1-ARRIVAL_RATE)\n",
    "                    if x_prime == x:\n",
    "                        #(0,0) + (1,1)\n",
    "                        P[x, x_prime, a] = (1-reduction_rate(a))*(1-ARRIVAL_RATE) + (reduction_rate(a))*(ARRIVAL_RATE)\n",
    "                    if x_prime == (x+1):\n",
    "                        #(0,1)\n",
    "                        P[x, x_prime, a] = (1-reduction_rate(a))*(ARRIVAL_RATE)\n",
    "    return P\n",
    "\n",
    "def transition_under_policy(pi):\n",
    "    P = transition_framework()\n",
    "\n",
    "    #create transition matrix based on policy P(x, x_prime)\n",
    "    P_pi = np.zeros((N_MAX,N_MAX))\n",
    "    for x in range(N_MAX):\n",
    "        for x_prime in [x-1, x, x+1]:\n",
    "            if (x_prime < N_MAX) and (x_prime >= 0):\n",
    "                for a in ACTION_LIST:\n",
    "                    P_pi[x,x_prime] += P[x,x_prime,a]*pi[x,a]\n",
    "    return P_pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation functions\n",
    "\n",
    "def value_under_policy(pi):\n",
    "    P = transition_framework()\n",
    "    r = reward_under_policy(pi)\n",
    "\n",
    "    X = np.size(P,1)\n",
    "    V_pi = np.zeros((X))\n",
    "\n",
    "    P_pi = transition_under_policy(pi)\n",
    "    V_pi = np.linalg.inv(np.eye(X) - GAMMA*P_pi).dot(r)\n",
    "    return V_pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulation functions\n",
    "\n",
    "def next_state(state_val, action_ind):\n",
    "    increment_t = random.choices([0, 1], weights=[1-ARRIVAL_RATE, ARRIVAL_RATE])[0]\n",
    "    decrement_t = random.choices([0, 1], weights=[1-reduction_rate(action_ind), reduction_rate(action_ind)])[0]\n",
    "    net_t = increment_t - decrement_t\n",
    "    return min(N_MAX-1, max(state_val + net_t, 0))\n",
    "\n",
    "def plot_results(data, x_title, y_title, title, color):\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.plot(data, linestyle = 'dotted', color = color)\n",
    "    plt.xlabel(x_title)\n",
    "    plt.ylabel(y_title)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature maps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#policies\n",
    "\n",
    "def policy_lazy():\n",
    "    pi = np.zeros((N_MAX,2))\n",
    "    for x in range(0,N_MAX):\n",
    "        pi[x, 0] = 1 #action_ind (low = 0, high = 1)\n",
    "        pi[x, 1] = 0\n",
    "    return pi\n",
    "\n",
    "def policy_aggr():\n",
    "    pi = np.zeros((N_MAX,2))\n",
    "    for x in range(0,N_MAX):\n",
    "        if x < 50:\n",
    "            pi[x, 0] = 1 #action_ind (low = 0, high = 1)\n",
    "            pi[x, 1] = 0\n",
    "        else:\n",
    "            pi[x, 0] = 0 \n",
    "            pi[x, 1] = 1 #action_ind (low = 0, high = 1)\n",
    "    return pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HW1 functions\n",
    "\n",
    "def value_iteration(max_iter):\n",
    "    #initialize frameworks \n",
    "    P = transition_framework()\n",
    "    r = np.zeros((N_MAX, 2))\n",
    "    for x in range(N_MAX):\n",
    "        for a in ACTION_LIST:\n",
    "            r[x,a] = reward_calc(x,a)\n",
    "\n",
    "    #initialize value function\n",
    "    V = np.zeros((N_MAX, 1))\n",
    "\n",
    "    #value iteration\n",
    "    for k in range(max_iter):\n",
    "        V_star = -np.inf\n",
    "        for a in range(2):\n",
    "            V_star = np.maximum(V_star, r[:, a].reshape([-1, 1]) + GAMMA* P[:, :, a].dot(V))\n",
    "        V = V_star\n",
    "    \n",
    "    return V.flatten()\n",
    "\n",
    "def policy_iteration(max_iter):\n",
    "\n",
    "    #initialize frameworks\n",
    "    P = transition_framework()\n",
    "    pi_star = policy_aggr()\n",
    "\n",
    "    #policy iteration\n",
    "    for k in range(max_iter):\n",
    "\n",
    "        V_new = value_under_policy(pi_star)\n",
    "\n",
    "        V_max = np.zeros((N_MAX,2))\n",
    "        for x in range(N_MAX):\n",
    "            for a in ACTION_LIST:\n",
    "                V_max[x,a] = reward_calc(x,a) + GAMMA*P[x, :, a].dot(V_new)\n",
    "\n",
    "        pi_star = np.eye(V_max.shape[1])[np.argmax(V_max, axis=1)]\n",
    "\n",
    "    return value_under_policy(pi_star), pi_star\n",
    "\n",
    "# pi_lazy = policy_lazy()\n",
    "# V_pi_lazy = value_under_policy(pi_lazy)\n",
    "# pi_aggr = policy_aggr()\n",
    "# V_pi_aggr = value_under_policy(pi_aggr)\n",
    "\n",
    "# fig, axes = plt.subplots(1, 4, figsize=(30, 6))\n",
    "\n",
    "# for i, ax in enumerate(axes):\n",
    "#     V_star_value = value_iteration(list_iters[i])\n",
    "#     V_star_policy, pi_star_policy = policy_iteration(list_iters[i])\n",
    "#     V_star_diff = V_star_value - V_star_policy\n",
    "\n",
    "#     # ax.plot(V_star_value, linestyle = 'dotted', color = 'green', label = 'Value')\n",
    "#     # ax.plot(V_star_policy, linestyle = 'dotted', color = 'blue', label = 'Policy')\n",
    "#     ax.plot(V_star_diff, linestyle = 'dotted', color = 'purple', label = 'Difference')\n",
    "#     ax.title.set_text(f'Compare Value and Policy Iteration:{list_iters[i]}')\n",
    "#     ax.legend()\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# fig, axes = plt.subplots(1, 4, figsize=(30, 6))\n",
    "\n",
    "# for i, ax in enumerate(axes):\n",
    "#     V_star_value = value_iteration(list_iters[i])\n",
    "#     V_star_diff_lazy = V_star_value - V_pi_lazy\n",
    "#     V_star_diff_aggr = V_star_value - V_pi_aggr\n",
    "\n",
    "#     ax.plot(V_star_diff_lazy, linestyle = 'dotted', color = 'red', label = 'Diff-Lazy')\n",
    "#     ax.plot(V_star_diff_aggr, linestyle = 'dotted', color = 'orange', label = 'Diff-Aggr')\n",
    "#     ax.title.set_text(f'Compare Value Iteration to Part1: {list_iters[i]}')\n",
    "#     ax.legend()\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='part1'>Part 1: Approximate Policy Evaluation</a>\n",
    "- **Objective**: Study the performance of simple policies using TD(0) and LSTD\n",
    "- **Tasks:**\n",
    "  - TD(0) algorithm creation\n",
    "  - LSTD algorithm creation\n",
    "  - Lazy policy (TD(0) and LSTD implementation)\n",
    "  - Aggressive policy (TD(0) and LSTD implementation)\n",
    "  - Evaluate efficacy under three feature maps with $10^4, 10^5, 10^6, 10^7$\n",
    "  - Plot the resulting value functions and compare to value function results from HW1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='part2'>Part 2: Approximate Policy Iteration</a>\n",
    "- **Objective**: implement an approximate policy iteration method\n",
    "- **Tasks:**\n",
    "  - Create approximate policy iteration method\n",
    "  - Plot value function after 10 and 100 iterations and compare to value function from Part1\n",
    "  - Compare the value function to the optimal value function from HW1\n",
    "  - Plot the service rates assigned by the final policy, interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
