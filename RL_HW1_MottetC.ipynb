{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - HW 1\n",
    "\n",
    "## by Clarice Mottet\n",
    "\n",
    "0. **[Part 0: Set Up](#part0)**\n",
    "- **Objective**: Initialize programming environment.\n",
    "- **Tasks:***\n",
    "    - Set up libraries\n",
    "    - Initialize global variables\n",
    "    - Create functions to be used throughout notebook base on problem set up\n",
    "\n",
    "1. **[Part 1: Policy Evaluation](#part1)**\n",
    "- **Objective**: Create policies and evaluate their efficacy\n",
    "- **Tasks:**\n",
    "  - Lazy policy\n",
    "  - Aggressive policy\n",
    "  - Evaluate efficacy by directly soluving the Bellman equations\n",
    "\n",
    "2. **[Part 2: Value Iteration and Policy Iteration](#part2)**\n",
    "- **Objective**: Calculate optimal policy using value iteration and policy iteration\n",
    "- **Tasks:**\n",
    "  - Value iteration: create algorithm and plot\n",
    "  - Policy iteration: create algorithm and plot\n",
    "  - Compare methods by plotting the differenc ebetween the optimal value function and value functions of the policies from problem 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='part0'>Part 0: Set Up</a>\n",
    "- **Objective**: Initialize programming environment.\n",
    "- **Tasks:**\n",
    "    - Set up libraries\n",
    "    - Initialize global variables\n",
    "    - Create functions to be used throughout notebook base on problem set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Set up libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_MAX = 100\n",
    "GAMMA = 0.9\n",
    "\n",
    "ACTION_LOW = 0.51\n",
    "ACTION_HIGH = 0.6\n",
    "\n",
    "COST_LOW = 0.0\n",
    "COST_HIGH = 0.01\n",
    "\n",
    "ARRIVAL_RATE = 0.5\n",
    "\n",
    "ACTION_LIST = [-1, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create functions to be used throughout notebook base on problem set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions\n",
    "\n",
    "#state_val is int between 0 and N_MAX\n",
    "#action_ind is 0 'low' or 1 'high'\n",
    "\n",
    "def reduction_rate(action_ind):\n",
    "    if action_ind == 0:\n",
    "        return ACTION_LOW\n",
    "    else:\n",
    "        return ACTION_HIGH\n",
    "\n",
    "def action_cost(action_ind):\n",
    "    if action_ind == 0:\n",
    "        return COST_LOW\n",
    "    else:\n",
    "        return COST_HIGH\n",
    "\n",
    "def reward_function(state_val, action_ind):\n",
    "    return -((state_val/N_MAX)**2) - action_cost(action_ind)\n",
    "\n",
    "def transition_function(state_val, action_ind):\n",
    "    increment_t = random.choices([0, 1], weights=[1-ARRIVAL_RATE, ARRIVAL_RATE])[0]\n",
    "    decrement_t = random.choices([0, 1], weights=[1-reduction_rate(action_ind), reduction_rate(action_ind)])[0]\n",
    "    net_t = increment_t - decrement_t\n",
    "    return min(N_MAX-1, max(state_val + net_t, 0))\n",
    "\n",
    "def policy_reward(pi):\n",
    "    r_pi = np.zeros(N_MAX)\n",
    "    for x in range(0, N_MAX):\n",
    "        action_ind = None\n",
    "        for a in range(0,2):\n",
    "            if pi[x,a] >= .5:\n",
    "                action_ind = a\n",
    "        r_pi[x] = reward_function(x, action_ind)\n",
    "    return r_pi\n",
    "\n",
    "def transition_maxtrix(pi):\n",
    "    #conditional matrix P(x, x_prime, a)\n",
    "    P = np.zeros((N_MAX,N_MAX,2))\n",
    "    for x in range(0,N_MAX):\n",
    "        for x_prime in range(0,N_MAX):\n",
    "            P[x, x_prime, 0] = ACTION_LOW\n",
    "            P[x, x_prime, 1] = ACTION_HIGH\n",
    "\n",
    "    #create transition matrix based on policy\n",
    "    P_pi = np.zeros((N_MAX,N_MAX))\n",
    "    for x in range(N_MAX):\n",
    "        for x_prime in range(N_MAX):\n",
    "            for a in range(0,2):\n",
    "                P_pi[x,x_prime] += P[x,x_prime,a]*pi[x,a]\n",
    "    return P_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='part1'>Part 1: Policy Evaluation</a>\n",
    "- **Objective**: Create policies and evaluate their efficacy\n",
    "- **Tasks:**\n",
    "  - Lazy policy\n",
    "  - Aggressive policy\n",
    "  - Evaluate efficacy by directly soluving the Bellman equations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lazy policy: policy that always uses the low servive rate.\n",
    "\n",
    "$$ \\pi_{lazy}(x) = a_{low} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P[x,y,a] gives the probability of moving from state x to state y when taking action a\n",
    "# pi[x,a] = probability of picking action a in state x (either 0 or 1)\n",
    "# P_pi[x,x']=P[x,x',pi[x]]\n",
    "# policy maps (x,a) pair to probability of taking action a in (scalarized) state x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create lazy policy function\n",
    "\n",
    "def policy_lazy():\n",
    "    pi = np.zeros((N_MAX,2))\n",
    "    for x in range(0,N_MAX):\n",
    "        pi[x, 0] = 1 #action_ind (low = 0, high = 1)\n",
    "        pi[x, 1] = 0\n",
    "    return pi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Aggressive policy: policy that uses the low service rate for short queues and the high service rate for long queues\n",
    "\n",
    "$$ \\pi_{aggr}(x) = \\begin{cases} a_{low}\\ \\text{if}\\ x < 50 \\\\ a_{high}\\ \\text{otherwise}\\ \\end{cases} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create aggresive policy function\n",
    "\n",
    "def policy_aggr():\n",
    "    pi = np.zeros((N_MAX,2))\n",
    "    for x in range(0,N_MAX):\n",
    "        if x < 50:\n",
    "            pi[x, 0] = 1 #action_ind (low = 0, high = 1)\n",
    "            pi[x, 1] = 0\n",
    "        else:\n",
    "            pi[x, 0] = 0 \n",
    "            pi[x, 1] = 1 #action_ind (low = 0, high = 1)\n",
    "    return pi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate efficacy by directly soluving the Bellman equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_lazy = policy_lazy()\n",
    "\n",
    "pi_aggr = policy_aggr()\n",
    "\n",
    "#get r_pi, P_pi and then use these to get V_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='part2'>Part 2: Value Iteration and Policy Iteration</a>\n",
    "- **Objective**: Calculate optimal policy using value iteration and policy iteration\n",
    "- **Tasks:**\n",
    "  - Value iteration: create algorithm and plot\n",
    "  - Policy iteration: create algorithm and plot\n",
    "  - Compare methods by plotting the differenc ebetween the optimal value function and value functions of the policies from problem 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
